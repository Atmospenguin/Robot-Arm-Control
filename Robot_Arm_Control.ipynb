{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Robotic Arm Control via Deep Reinforcement Learning\n"
      ],
      "metadata": {
        "id": "8GRX-VCccyuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will guide you through:\n",
        "\n",
        "1.  **Setting up the Environment:** Installing necessary libraries.\n",
        "2.  **Choosing a Simulator:** Using PyBullet via the `panda-gym` environment, which integrates with Gymnasium (formerly OpenAI Gym).\n",
        "3.  **Selecting an RL Algorithm:** Using Soft Actor-Critic (SAC) from Stable Baselines3 (SB3), a state-of-the-art algorithm suitable for continuous control tasks like robotics.\n",
        "4.  **Training the Agent:** Running the training loop.\n",
        "5.  **Evaluating the Agent:** Visualizing the trained agent's performance.\n",
        "\n",
        "---\n",
        "\n",
        "**Google Colab Notebook: Robotic Arm Control with Reinforcement Learning**\n",
        "\n",
        "---\n",
        "\n",
        "**1. Introduction**\n",
        "\n",
        "This notebook demonstrates how to train a Reinforcement Learning (RL) agent to control a simulated robotic arm. We will use the `panda-gym` environment, which simulates a Franka Emika Panda arm using the PyBullet physics engine. The goal will be a basic reaching task, where the arm must move its end-effector to a target position. We will use the Stable Baselines3 library to implement the Soft Actor-Critic (SAC) algorithm.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "* **Environment:** `panda-gym` (using PyBullet & Gymnasium API)\n",
        "* **RL Library:** Stable Baselines3 (SB3)\n",
        "* **Algorithm:** Soft Actor-Critic (SAC)\n",
        "* **Task:** `PandaReach-v3` (or similar)\n",
        "\n",
        "---\n",
        "\n",
        "**2. Setup and Installations**\n",
        "\n",
        "First, we need to install the required libraries. This includes Gymnasium (the environment API), `panda-gym` (the specific robot environment), PyBullet (the physics simulator), Stable Baselines3 (the RL algorithms), and utilities for rendering in Colab."
      ],
      "metadata": {
        "id": "Lzgl8jZUcsTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Dependencies\n",
        "# Stable Baselines3 for RL algorithms\n",
        "!pip install stable-baselines3[extra]>=2.0.0a5 --quiet\n",
        "\n",
        "# Gymnasium for the environment API\n",
        "!pip install gymnasium --quiet\n",
        "\n",
        "# PyBullet for physics simulation\n",
        "!pip install pybullet --quiet\n",
        "\n",
        "# Panda-Gym for the specific robotic arm environment\n",
        "# Note: Might require specific versions depending on compatibility. Check the panda-gym repo if issues arise.\n",
        "!pip install panda-gym --quiet\n",
        "\n",
        "# For rendering environments in Colab\n",
        "!pip install pyglet==1.5.27 --quiet # Specific version often needed for compatibility in Colab\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "!apt-get update --quiet\n",
        "!apt-get install -y xvfb python-opengl ffmpeg --quiet\n",
        "\n",
        "print(\"âœ… Dependencies installed.\")\n",
        "\n",
        "# Set up a virtual display for rendering\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "print(\"âœ… Virtual display started.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lrqDyX0tcsTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**3. Import Libraries**\n",
        "\n",
        "Now, let's import the necessary Python libraries."
      ],
      "metadata": {
        "id": "deLlnPl9csTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import Libraries\n",
        "import gymnasium as gym\n",
        "import panda_gym\n",
        "import numpy as np\n",
        "import stable_baselines3 as sb3\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "print(f\"Gymnasium version: {gym.__version__}\")\n",
        "print(f\"Stable Baselines3 version: {sb3.__version__}\")\n",
        "print(\"âœ… Libraries imported.\")\n",
        "\n",
        "# Helper function to display recorded videos\n",
        "def show_video(video_path):\n",
        "    \"\"\"Helper function to display mp4 video in Colab\"\"\"\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wTYWvdWUcsTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**4. Environment Setup**\n",
        "\n",
        "We'll instantiate the robotic arm environment. We'll use `PandaReach-v3`, where the objective is to move the arm's end-effector to a randomly placed target sphere. We wrap it with `Monitor` to log training statistics like rewards.\n",
        "\n",
        "* **Observation Space:** Typically includes joint positions, velocities, and the target position.\n",
        "* **Action Space:** Continuous values representing the desired change in joint positions or target end-effector velocity. `panda-gym` usually uses end-effector velocity control."
      ],
      "metadata": {
        "id": "XgSdKQtTcsTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Environment\n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Environment ID\n",
        "env_id = 'PandaReach-v3' # Try 'PandaPickAndPlace-v3', 'PandaPush-v3', 'PandaSlide-v3' for more complex tasks later\n",
        "\n",
        "# Create and wrap the environment\n",
        "# Use render_mode='rgb_array' for capturing frames\n",
        "env = gym.make(env_id, render_mode='rgb_array')\n",
        "env = Monitor(env, log_dir) # Wrap with Monitor to log stats\n",
        "\n",
        "print(f\"âœ… Environment '{env_id}' created.\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Action space sample: {env.action_space.sample()}\") # Example random action\n",
        "\n",
        "# Optional: Test the environment with random actions\n",
        "# obs, _ = env.reset()\n",
        "# frame = env.render()\n",
        "# plt.imshow(frame)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "# action = env.action_space.sample()\n",
        "# obs, reward, terminated, truncated, info = env.step(action)\n",
        "# print(f\"Step result: Obs shape: {obs.shape}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
        "# env.close() # Close the env if just testing"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "800Nh9rVcsTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**5. Define RL Agent (SAC)**\n",
        "\n",
        "We choose the Soft Actor-Critic (SAC) algorithm from Stable Baselines3. SAC is well-suited for continuous control problems like this one. It uses an entropy maximization framework which encourages exploration and generally leads to robust policies.\n",
        "\n",
        "We use the `MlpPolicy`, which means the policy and value functions will be represented by Multi-Layer Perceptrons (MLPs)."
      ],
      "metadata": {
        "id": "2SUU5bjgcsTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the SAC Agent\n",
        "\n",
        "# Use VecEnv for potential parallelization (even with n_envs=1)\n",
        "# vec_env = make_vec_env(lambda: Monitor(gym.make(env_id, render_mode='rgb_array'), log_dir), n_envs=1)\n",
        "\n",
        "# Define the SAC agent\n",
        "# - 'MlpPolicy': Use standard Multi-Layer Perceptron policy\n",
        "# - env: The environment instance (or VecEnv)\n",
        "# - verbose=1: Print training progress\n",
        "# - learning_rate: How quickly the agent updates its policy (can be tuned)\n",
        "# - buffer_size: Size of the replay buffer (stores past experiences)\n",
        "# - batch_size: Number of samples used for each gradient update\n",
        "# - gamma: Discount factor for future rewards\n",
        "# - tau: Soft update coefficient for target networks\n",
        "# - learning_starts: How many steps to collect before starting training\n",
        "# - tensorboard_log: Directory for TensorBoard logs\n",
        "model = SAC('MultiInputPolicy',\n",
        "            env,\n",
        "            verbose=1,\n",
        "            learning_rate=1e-4,  # Often needs tuning, default is 3e-4\n",
        "            buffer_size=100_000, # Smaller buffer for faster iteration initially\n",
        "            batch_size=256,\n",
        "            gamma=0.98,        # Discount factor often lower in goal-based envs\n",
        "            tau=0.01,\n",
        "            learning_starts=1000, # Start learning after collecting some experience\n",
        "            tensorboard_log=\"./sac_panda_tensorboard/\",\n",
        "            seed=42) # for reproducibility\n",
        "\n",
        "print(\"âœ… SAC Agent defined.\")\n",
        "print(f\"Policy Architecture: {model.policy}\")\n",
        "\n",
        "# Optional: You can load a pre-trained model here if you have one\n",
        "# model_path = \"sac_panda_reach_100k.zip\"\n",
        "# if os.path.exists(model_path):\n",
        "#    print(f\"Loading pre-trained model from {model_path}\")\n",
        "#    model = SAC.load(model_path, env=env)\n",
        "# else:\n",
        "#    print(\"No pre-trained model found, starting training from scratch.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a2JjExU5csTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**6. Train the Agent**\n",
        "\n",
        "Now we train the agent using the `model.learn()` method. We specify the total number of timesteps for training. During training, SB3 will print logs showing the progress (episode reward, episode length, etc.)."
      ],
      "metadata": {
        "id": "vm9U_zQXcsTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train the Agent\n",
        "# Define training parameters\n",
        "TOTAL_TIMESTEPS = 50000 # Start with a moderate number (e.g., 50k-100k). Increase for better performance.\n",
        "MODEL_SAVE_PATH = f\"sac_{env_id.split('-')[0].lower()}_{TOTAL_TIMESTEPS//1000}k\"\n",
        "\n",
        "print(f\"ðŸš€ Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
        "print(f\"TensorBoard logs will be saved in: {model.tensorboard_log}\")\n",
        "print(f\"Model will be saved to: {MODEL_SAVE_PATH}.zip\")\n",
        "\n",
        "# Add a callback for plotting reward progress (optional but helpful)\n",
        "class PlottingCallback(BaseCallback):\n",
        "    def __init__(self, log_dir, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Log scalar value (here we log the number of steps)\n",
        "        # More complex logging (like mean reward) is handled by Monitor wrapper + TensorBoard\n",
        "        # self.logger.record('custom/steps', self.num_timesteps)\n",
        "\n",
        "        # Check if Monitor has enough data to calculate mean reward\n",
        "        if self.num_timesteps % 1000 == 0: # Check every 1000 steps\n",
        "             x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "             if len(x) > 0:\n",
        "                 # Mean reward over last 100 episodes\n",
        "                 mean_reward = np.mean(y[-100:])\n",
        "                 if self.verbose > 0:\n",
        "                     print(f\"Num timesteps: {self.num_timesteps}, Mean reward (last 100): {mean_reward:.2f}\")\n",
        "                 # Simple best model saving based on mean reward\n",
        "                 if mean_reward > self.best_mean_reward:\n",
        "                     self.best_mean_reward = mean_reward\n",
        "                     # Example saving logic (optional)\n",
        "                     # print(f\"New best mean reward: {self.best_mean_reward:.2f} - Saving model\")\n",
        "                     # self.model.save(self.save_path)\n",
        "             else:\n",
        "                  if self.verbose > 0:\n",
        "                      print(f\"Num timesteps: {self.num_timesteps}, Not enough data for mean reward yet.\")\n",
        "\n",
        "        return True # Continue training\n",
        "\n",
        "# Create the callback\n",
        "plot_callback = PlottingCallback(log_dir=log_dir, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "# Note: Training robotics tasks can take time! Start with fewer timesteps.\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEPS,\n",
        "            log_interval=10, # Log stats every 10 episodes\n",
        "            tb_log_name=f\"SAC_{env_id}\",\n",
        "            callback=plot_callback,\n",
        "            reset_num_timesteps=False) # Set to False if continuing training\n",
        "\n",
        "print(f\"âœ… Training finished after {TOTAL_TIMESTEPS} timesteps.\")\n",
        "\n",
        "# Save the final model\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"âœ… Model saved to {MODEL_SAVE_PATH}.zip\")\n",
        "\n",
        "# You can launch TensorBoard in Colab (might need a specific cell)\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir ./sac_panda_tensorboard/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xqq_NrpTcsTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**7. Evaluate and Visualize the Trained Agent**\n",
        "\n",
        "After training, let's see how well the agent performs. We'll run the agent in the environment for a few episodes and record a video of its behavior."
      ],
      "metadata": {
        "id": "fdixhi37csTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate the Trained Agent and Record Video\n",
        "\n",
        "# --- Load the trained model (if needed) ---\n",
        "# If you didn't just train, load the model:\n",
        "# model_path = f\"{MODEL_SAVE_PATH}.zip\"\n",
        "# if os.path.exists(model_path):\n",
        "#     print(f\"Loading model from {model_path}\")\n",
        "#     model = SAC.load(model_path, env=env) # Make sure env is defined\n",
        "# else:\n",
        "#     print(\"Model file not found. Cannot evaluate.\")\n",
        "#     # exit() # Or handle appropriately\n",
        "\n",
        "# --- Evaluation and Recording ---\n",
        "print(\"Evaluating the trained agent...\")\n",
        "\n",
        "# Create a separate evaluation environment if needed (or reuse 'env')\n",
        "eval_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "# Record a video\n",
        "video_folder = 'logs/videos/'\n",
        "video_length = 500 # Number of steps to record\n",
        "\n",
        "# Wrap the environment for video recording\n",
        "eval_env = gym.wrappers.RecordVideo(eval_env, video_folder=video_folder,\n",
        "                                    episode_trigger=lambda e: True, # Record every episode\n",
        "                                    name_prefix=f\"{MODEL_SAVE_PATH}-eval\")\n",
        "eval_env.reset() # Reset before starting recording\n",
        "\n",
        "obs, _ = eval_env.reset()\n",
        "cumulative_reward = 0\n",
        "frames = [] # Store frames if needed for other viz\n",
        "\n",
        "print(\"Starting video recording...\")\n",
        "for _ in range(video_length):\n",
        "    action, _states = model.predict(obs, deterministic=True) # Use deterministic actions for evaluation\n",
        "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
        "    cumulative_reward += reward\n",
        "\n",
        "    # frame = eval_env.render() # Handled by RecordVideo wrapper now\n",
        "    # frames.append(frame)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(f\"Episode finished. Cumulative Reward: {cumulative_reward}\")\n",
        "        obs, _ = eval_env.reset()\n",
        "        cumulative_reward = 0\n",
        "\n",
        "eval_env.close() # Important to save the video file properly\n",
        "print(f\"âœ… Evaluation finished. Video saved in {video_folder}\")\n",
        "\n",
        "# --- Display the Recorded Video ---\n",
        "# Find the latest video file\n",
        "video_files = sorted([os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.mp4')])\n",
        "if video_files:\n",
        "    latest_video = video_files[-1]\n",
        "    print(f\"Displaying video: {latest_video}\")\n",
        "    display(show_video(latest_video))\n",
        "else:\n",
        "    print(\"Could not find a recorded video file.\")\n",
        "\n",
        "# --- Plot Training Curve ---\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "    :param log_folder: the save location of the results to plot\n",
        "    :param title: the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=50) # Smooth the curve\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "print(\"\\nPlotting training results...\")\n",
        "plot_results(log_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "uJeQpHUxcsTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**8. Further Steps and Ideas**\n",
        "\n",
        "* **Tune Hyperparameters:** Experiment with `learning_rate`, `buffer_size`, `batch_size`, `gamma`, network architecture (e.g., `net_arch=[256, 256]` in SAC policy_kwargs).\n",
        "* **Try Different Tasks:** Use other `panda-gym` environments like `PandaPickAndPlace-v3`, `PandaPush-v3`, or `PandaSlide-v3`. These are significantly harder and require more training time and potentially different reward structures or hyperparameters.\n",
        "* **Use Different Algorithms:** Try other SB3 algorithms like TD3 (Twin Delayed DDPG) or PPO (Proximal Policy Optimization)."
      ],
      "metadata": {
        "id": "G3BPBvs3csTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using TD3\n",
        "    # from stable_baselines3 import TD3\n",
        "    # model = TD3('MlpPolicy', env, verbose=1, tensorboard_log=\"./td3_panda_tensorboard/\")\n",
        "    # model.learn(total_timesteps=TOTAL_TIMESTEPS)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Lmiv3exvcsTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Improve Reward Shaping:** For complex tasks, the default reward might not be sufficient. Designing a better reward function (reward shaping) can significantly speed up learning but requires careful consideration to avoid unintended behaviors.\n",
        "* **Domain Randomization:** If aiming for transfer to a real robot, train with variations in physics parameters (mass, friction, etc.) to make the policy more robust.\n",
        "* **Custom Environments:** Create your own tasks or use different robot models by building custom Gymnasium environments using PyBullet or other simulators like MuJoCo.\n",
        "* **Explore Advanced Techniques:** Hierarchical RL, Imitation Learning (learning from demonstrations), Offline RL (learning from pre-collected datasets).\n",
        "\n",
        "---\n",
        "\n",
        "**9. Resources**\n",
        "\n",
        "* **Stable Baselines3 Documentation:** [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "* **Gymnasium Documentation:** [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
        "* **Panda-Gym Repository:** [https://github.com/qgallouedec/panda-gym](https://github.com/qgallouedec/panda-gym)\n",
        "* **PyBullet:** [https://pybullet.org/](https://pybullet.org/)\n",
        "* **Soft Actor-Critic Paper:** [https://arxiv.org/abs/1801.01290](https://arxiv.org/abs/1801.01290) (Original SAC) and [https://arxiv.org/abs/1812.05905](https://arxiv.org/abs/1812.05905) (SAC with automatic temperature tuning)\n",
        "\n",
        "---\n",
        "\n",
        "This Colab notebook provides a starting point. Remember that training RL agents for robotics can be computationally intensive and require significant tuning to achieve good performance, especially on complex tasks. Good luck!"
      ],
      "metadata": {
        "id": "SSrYhx2ZcsTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/DLR-RM/stable-baselines3/issues/819\">https://github.com/DLR-RM/stable-baselines3/issues/819</a></li>\n",
        "  <li><a href=\"https://developer.aliyun.com/ask/548708\">https://developer.aliyun.com/ask/548708</a></li>\n",
        "  <li><a href=\"https://github.com/Jason-CKY/DeepRL-pytorch\">https://github.com/Jason-CKY/DeepRL-pytorch</a></li>\n",
        "  <li><a href=\"https://github.com/pypi-diff/20240413\">https://github.com/pypi-diff/20240413</a></li>\n",
        "  <li><a href=\"https://github.com/Turbo503/Deep-Wave-Trader\">https://github.com/Turbo503/Deep-Wave-Trader</a></li>\n",
        "  <li><a href=\"https://github.com/araffin/rl-baselines-zoo\">https://github.com/araffin/rl-baselines-zoo</a> subject to MIT</li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "hb_uvHi1csTt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}