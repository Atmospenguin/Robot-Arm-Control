{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e88f05",
   "metadata": {},
   "source": [
    "# Robotic Arm Control via Deep Reinforcement Learning (SAC + PyBullet + panda-gym)\n",
    "This notebook demonstrates how to train a Soft Actor‑Critic (SAC) agent to control a Panda robotic arm in a PyBullet simulation via the **panda‑gym** environments.\n",
    "\n",
    "**Sections**\n",
    "1. Install & import libraries\n",
    "2. Create the vectorised simulation environment\n",
    "3. Train an SAC agent (with TensorBoard logging)\n",
    "4. Evaluate & (optionally) record a demo video\n",
    "5. Inspect results in TensorBoard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4d2e4",
   "metadata": {},
   "source": [
    "## 1  Install & import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q gymnasium==0.29.1 panda-gym==3.0.1 pybullet==3.2.6 stable-baselines3==2.4.0 sb3-contrib==2.4.0 tensorboard\n",
    "# After the first run you can comment‑out the cell above to avoid re‑installing.\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import make_vec_env, SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os, datetime, getpass, shutil\n",
    "print('All libraries imported ✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2453ea",
   "metadata": {},
   "source": [
    "## 2  Create the simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task (dense reward versions learn faster)\n",
    "ENV_ID = 'PandaReachDense-v3'  # e.g. PandaSlideDense-v3, PandaPickAndPlaceDense-v3, …\n",
    "N_ENVS = 4                     # Parallel environments (adjust to your CPU cores)\n",
    "SEED   = 42\n",
    "\n",
    "log_dir = './logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    lambda: Monitor(gym.make(ENV_ID, render_mode=None)),\n",
    "    n_envs=N_ENVS,\n",
    "    seed=SEED,\n",
    "    vec_env_cls=SubprocVecEnv,   # runs each env in its own process\n",
    ")\n",
    "vec_env.reset()\n",
    "print(f'Vectorised environment `{ENV_ID}` with {N_ENVS} workers ready ✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c78c61",
   "metadata": {},
   "source": [
    "## 3  Train the SAC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './sac_panda_reach'\n",
    "TIMESTEPS  = 100_000          # adjust as needed (≥1 M for best performance)\n",
    "\n",
    "model = SAC(\n",
    "    policy='MlpPolicy',\n",
    "    env=vec_env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_dir,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=TIMESTEPS, progress_bar=True)\n",
    "model.save(MODEL_PATH)\n",
    "print(f'Model saved to {MODEL_PATH} ✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616315c",
   "metadata": {},
   "source": [
    "## 4  Evaluate the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(ENV_ID, render_mode='human')  # change to 'rgb_array' for headless servers\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean reward over 10 episodes: {mean_reward:.2f} ± {std_reward:.2f}')\n",
    "\n",
    "# Optional: record a short video\n",
    "# from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "# video_folder = './videos/'\n",
    "# os.makedirs(video_folder, exist_ok=True)\n",
    "# video_env = VecVideoRecorder(eval_env, video_folder,\n",
    "#                              record_video_trigger=lambda ep: ep == 0,\n",
    "#                              video_length=500, name_prefix='sac-demo')\n",
    "# video_env.reset()\n",
    "# for _ in range(500):\n",
    "#     action, _ = model.predict(video_env.reset()[0], deterministic=True)\n",
    "#     video_env.step(action)\n",
    "# video_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678911aa",
   "metadata": {},
   "source": [
    "## 5  Inspect results in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, run the two magic commands below **in a new cell**\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447afb76",
   "metadata": {},
   "source": [
    "## 6  Shut down & clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.close()\n",
    "print('Training session finished — environment closed.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
